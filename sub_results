score	model				            data		    last step	paras			        split_num	dropout
                                                        model
-------------guoday------------
0.7999	RoBERTa_large+guoday		    original	    False		epoch3 _bs64_lr5e-5	    1		    0.1
0.8112	RoBERTa_large+guoday		    original	    False		epoch3 _bs64_lr5e-5	    3		    0.1
0.8077
0.8030	RoBERTa_large+guoday            original        False   	epoch3 _bs64_lr5e-5     3           0.2
0.7991	RoBERTa_large+guoday            original        False   	epoch10_bs64_lr5e-5     1		    0.1
0.8020	RoBERTa_large+guoday            original        False   	epoch10_bs64_lr5e-5     3		    0.1
0.8098	RoBERTa_large+guoday            original        False   	epoch5 _bs128_lr5e-5    3           0.1	# epoch num matters?
0.7873	RoBERTa_large+guoday            original        False   	epoch3 _bs32_lr5e-5     3		    0.1	# batch size matters?
0.8060	RoBERTa_large+guoday            original        False   	epoch3 _bs128_lr5e-5    3           0.1
0.8071
0.8035	RoBERTa_large+guoday            original        False   	epoch5 _bs128_lr5e-6	3		    0.1	# learning rate matters?

-------------MLP---------------
0.8059	RoBERTa_large+MLP		        original        False   	epoch3_bs128_lr5e-5     3		    0.1	# switch guoday to MLP
0.8005	RoBERTa_large+MLP		        original        False   	epoch3_bs128_lr1e-5     3		    0.2
0.7922	RoBERTa_large+MLP               original        False   	epoch5_bs128_lr1e-5     3           0.2
0.8021	RoBERTa_large+MLP               original        False   	epoch3_bs128_lr1e-5     1           0.1

-----------GRU_MLP-------------
0.8052	RoBERTa_large+GRU_MLP           original        False   	epoch3_bs128_lr5e-5     1		    0.1	# GRU_MLP
0.8104	RoBERTa_large+GRU_MLP           original        False   	epoch10_bs128_lr5e-5    1           0.1
0.8071	RoBERTa_large+GRU_MLP           original        False   	epoch3_bs128_lr5e-5     3           0.1

0.8057  RoBERTa_large+GRU_MLP           original        False   	epoch8_bs128_lr5e-5	    3           0.1
0.7963  RoBERTa_large+GRU_MLP           original        True		epoch8_bs128_lr5e-5     3           0.1

0.8039  RoBERTa_large+GRU_MLP           clean           True		epoch8_bs128_lr5e-5     3           0.1
0.7935  RoBERTa_large+GRU_MLP           clean           False		epoch8_bs128_lr5e-5     3           0.1

0.7726  RoBERTa_large+MLP		        clean&summary   False   	epoch8_bs128_lr5e-5     1		    0.1
as aboveRoBERTa_large+MLP		        clean&summary   True        epoch8_bs128_lr5e-5     1		    0.1

0.8011  RoBERTa_large+GRU_MLP_256l      original        False   	epoch8_bs64_lr5e-5      1           0.1
0.7972  RoBERTa_large+GRU_MLP_256l      original        True		epoch8_bs64_lr5e-5      1           0.1

0.7999  RoBERTa_large+GRU_MLP_256l      original        False   	epoch8_bs128_lr5e-5	    1           0.3
to test RoBERTa_large+GRU_MLP_256l      original        True		epoch8_bs128_lr5e-5     1           0.3

0.7839  RoBERTa_large+GRU_MLP           balanced v2     False   	epoch5_bs128_lr5e-5	    3           0.1
0.7885  RoBERTa_large+GRU_MLP           balanced v2     True		epoch5_bs128_lr5e-5     3           0.1

0.7969  RoBERTa_large+GRU_MLP           balanced v2     False   	epoch5_bs128_lr5e-5	    1           0.1
        RoBERTa_large+GRU_MLP           balanced v2     True		epoch5_bs128_lr5e-5     1           0.1

0.8045  RoBERTa_large+GRU_MLP           ba2 except eval False       epoch3_bs128_lr5e-5     3           0.1

0.8098	RoBERTa_large_wwm+GRU_MLP       original        False   	epoch3_bs128_lr5e-5     1		    0.1

0.8031  RoBERTa_large+GRU_MLP_warm      myba except evalFalse       epoch3_bs64_lr5e-5      3           0.1

0.8114  RoBERTa_large+GRU_MLP           ba except eval  False       epoch3_bs128_lr5e-5     3           0.1

0.8026  RoBERTa_large+GRU_MLP           myba except evalFalse       epoch3_bs128_lr5e-5     3           0.1

0.8079  My_roberta_20000+GRU_MLP        myba except evalFalse       epoch3_bs128_lr5e-5     3           0.1

0.8044  My_roberta_2000+GRU_MLP         myba except evalFalse       epoch3_bs128_lr5e-5     3           0.1

0.8106  RoBERTa_large_wwm+GRU_MLP       ba except eval  False       epoch3_bs128_lr5e-5     3           0.1

0.8082  My_roberta_12000_bri+GRU_MLP    ba except eval  False       epoch3_bs128_lr5e-5     3           0.1

to trainMy_roberta_12000_hit+GRU_MLP    ba except eval  False       epoch3_bs128_lr5e-5     3           0.1

-----------GRU_highway----------
0.8059  RoBERTa_large+GRU_highway       original        epoch3_bs128_lr5e-5     1           0.2

0.7923  RoBERTa_large+GRU_highway       original        epoch3_bs128_lr5e-5     1           0.2

0.8040  RoBERTa_large+GRU_highway       ba except eval  epoch3_bs128_lr5e-5	    3           0.1

0.8132  RoBERTa_large_hit+GRU_highway   ba except eval  epoch3_bs128_lr5e-5     3           0.1

0.8052  my_roberta_large_500_tiny_hit   ba except eval  epoch3_bs128_lr5e-5     3           0.1     new data_loader

0.8146  RoBERTa_large_hit+GRU_highway   ba except eval  epoch3_bs128_lr2e-5     3           0.1     new data_loader

0.8161  Roberta_large_hit+guo_MLP       ba except eval  epoch4_bs128_lr2e-5     3           0.1     new data_loader

to test roberta_large_hit+guo_MLP       ba except eval  epoch4_bs128_lr2e-5     5           0.1     new data_loaderv2+wrong data del

to trainroberta_large_hit+              new data_loaderv2+wrong data del+RAdam

-------------------ensemble results------------------

probs:0.8176    vote:0.8164     [8106, 8112, 8114]
probs:0.8156    vote:           [8106, 8112, 8114, 8114]
probs:0.8202    vote:0.8195     [8112, 8114, 8132]
probs:0.8240    vote:0.8175     [8112, 8114, 8132, 8132]
probs:0.8188    vote:           [8106, 8112, 8114, 8132]
probs:0.8208    vote:           [8106, 8112, 8114, 8132, 8132]
probs:0.8214    vote:           [8112, 8114, 8132, 8132, 8132]
probs:0.8237    vote:           [8112, 8114, 8132, 8132, 8146, 8146]
probs:0.8173    vote:           [8132, 8146, 8161]
probs:0.8200    vote:           [8112, 8114, 8132, 8146, 8161]

pay attention to: RAdam split_num_2   dout:0.0  epoch5 lr:2e-5 guoday's training speed    
title+content in split 1 but only content in other splits when not guoday's.

-----------------------------------------LEADERBOARD B------------------------------------------

------------------single model results---------------
to test roberta_large_hit+GRU_highway   train_v1    ba except eval  epoch3_bs128_lr5e-5     3   0.1
to test roberta_large_hit+GRU_highway   train_v1    ba except eval  epoch3_bs128_lr2e-5     3   0.1 new data_loader
to test roberta_large_hit+guo_MLP       train_v1    ba except eval  epoch4_bs128_lr2e-5     3   0.1 new data_loader
to test roberta_large_hit+guo_MLP       train_v1    ba except eval  epoch4_bs128_lr2e-5     5   0.1 new data_loaderv2+wrong data del

to trainroberta_large_hit+GRU_highway   train_v1v2  ba except eval  epoch4_bs128_lr3e-5     3   0.1 new data_loaderv2+wrong data del
to trainroberta_large_hit+guo_MLP       train_v1v2  ba except eval  epoch4_bs128_lr3e-5     3   0.1 new data_loaderv2+wrong data del
0.8073  roberta_large_hit+guo_MLP       train_v1v2  ba except eval  epoch4_bs128_lr2e-5     5   0.1 new data_loaderv2+wrong data del

to train    RAdam
to train    GRU LSTM...roberta_large_hit roberta_large_brightmart

-------------------ensemble results------------------
probs:



