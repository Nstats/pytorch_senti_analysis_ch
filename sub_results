score	model				data		last step model	paras			split_num	dropout
-------------guoday------------
0.7999	RoBERTa_large+guoday		original	False		epoch3 _bs64_lr5e-5	1		0.1
0.8112	RoBERTa_large+guoday		original	False		epoch3 _bs64_lr5e-5	3		0.1
0.8077
0.8030	RoBERTa_large+guoday            original        False   	epoch3 _bs64_lr5e-5     3               0.2
0.7991	RoBERTa_large+guoday            original        False   	epoch10_bs64_lr5e-5     1		0.1
0.8020	RoBERTa_large+guoday            original        False   	epoch10_bs64_lr5e-5     3		0.1
0.8098	RoBERTa_large+guoday            original        False   	epoch5 _bs128_lr5e-5    3               0.1	# epoch num matters?
0.7873	RoBERTa_large+guoday            original        False   	epoch3 _bs32_lr5e-5     3		0.1	# batch size matters?
0.8060	RoBERTa_large+guoday            original        False   	epoch3 _bs128_lr5e-5    3               0.1
0.8071
0.8035	RoBERTa_large+guoday            original        False   	epoch5 _bs128_lr5e-6	3		0.1	# learning rate matters?

-------------MLP---------------
0.8059	RoBERTa_large+MLP		original        False   	epoch3_bs128_lr5e-5     3		0.1	# switch guoday to MLP
0.8005	RoBERTa_large+MLP		original        False   	epoch3_bs128_lr1e-5     3		0.2
0.7922	RoBERTa_large+MLP               original        False   	epoch5_bs128_lr1e-5     3               0.2	
0.8021	RoBERTa_large+MLP               original        False   	epoch3_bs128_lr1e-5     1               0.1

-----------GRU_MLP-------------
0.8052	RoBERTa_large+GRU_MLP           original        False   	epoch3_bs128_lr5e-5     1		0.1	# GRU_MLP
0.8104	RoBERTa_large+GRU_MLP           original        False   	epoch10_bs128_lr5e-5    1               0.1
0.8071	RoBERTa_large+GRU_MLP           original        False   	epoch3_bs128_lr5e-5     3               0.1
	RoBERTa_large+GRU_MLP           original        False   	epoch8_bs128_lr5e-5	3               0.1
	RoBERTa_large+GRU_MLP           original        True		epoch8_bs128_lr5e-5     3               0.1
